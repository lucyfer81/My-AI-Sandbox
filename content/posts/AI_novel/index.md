+++
title = 'AI会获得诺贝尔文学奖吗?'
date = 2025-08-18T10:00:11+08:00
draft = false
+++

**大家好，我是老V。一个非常不专业的AI不极客。**

今天一大早六点就醒了。大概年纪大了，便睡不着了。于是也不强求，掏出手机和Kimi开始聊天。自从Kimi升级以后，我发现它自信了不少。日常输出的句式就是，要么是三招帮你搞定xx的一整套流程。要么就是，一句话总结，XXXXXX。不管准确不准确，先把自己知识渊博的AI设立住。

总所周知，Chatgpt上下文达到了128K，Claude更多，Gemini更是号称1M的上下文。我和Kimi详细请教了有关大模型的所谓上下文的定义和计算方式。

简单来说，目前主流的 AI 大模型在计算上下文窗口时，**把输入（prompt）和输出（completion）的 token 数量都计算在内**。也就是说，上下文窗口的长度限制是一个**总长度上限**，包括：

- 用户输入的 token（包括系统提示、历史对话等）
    
- 模型生成的 token（即输出）
    

举个例子：

- 如果一个模型的上下文窗口是 **128k tokens**（比如 GPT-4-turbo-128k）
    
- 你输入了 **100k tokens** 的内容（包括提示和对话历史）
    
- 那么模型最多只能再生成 **28k tokens** 的输出，否则会触发“超出上下文长度”的错误
    

这也是为什么在实际使用中，**输入越长，可输出的内容就越短**。

但是，不同 AI 模型**对输入和输出的 token 限制并不相同**，它们在**总上下文长度**和**单次最大输出长度**也会做出各自的限制，比如：

### ✅ 1. **总上下文长度 ≠ 输入+输出可自由分配**

虽然大多数模型将**输入 + 输出 token 总和限制在一个固定上限内**，但部分模型对**输入**和**输出**有**额外限制**，并不完全自由分配。就比如：


| 模型/平台             | 总上下文长度      | 输入上限       | 输出上限             | 说明                    |
| :---------------- | :---------- | :--------- | :--------------- | :-------------------- |
| **Claude 3.7**    | 204K tokens | 无单独限制      | 无单独限制            | 输入+输出总和不超过204K        |
| **GPT-4-turbo**   | 128K tokens | 无单独限制      | 默认4K，可调至最大8K或16K | 输出受 `max_tokens` 参数限制 |
| **DeepSeek V3**   | 64K tokens  | 56K tokens | 8K tokens        | 明确划分输入与输出上限           |



---

### ✅ 2. **部分模型对输出长度有硬性上限**

即便总上下文很长，**输出也可能被单独限制**，比如：

    
- **DeepSeek V3**：明确限制输出最多 8K tokens。
    
- **GPT-4o mini**：输出上限为 16K tokens，即使上下文为 128K。

好了，于是这勾起了我的兴趣。比如，GPT4支持128K的上下文，大约96K汉字。Claude 支持200K的上下文，输出上线128K，那如果是不是可以让它门来创作短篇小说呢？

于是我又和Kimi聊起了这个话题。不得不说，Kimi装专家的范儿缺失足，它给我的答复是：

|模型|适合题材|输出上限|对话体验|调用入口|
|:--|:--|:--|:--|:--|
|**Kimi-K2**|爽文、都市、群像|128 k token|中文语感最好，多轮续写最连贯|kimi.moonshot.cn / API|
|**Claude 3.7**|情感、群像、悬疑|128 k token|人物心理细腻，逻辑严谨|Anthropic 官方 / Claude Code|
|**GPT-4-turbo**|悬疑、推理、科幻设定|4 k~16 k token|结构清晰，文风偏“标准”|OpenAI API / ChatGPT Plus|
|**DeepSeek-R1**|科幻、脑洞、悬疑|8 k token（单次）|深度思考模式创意炸裂，但长文需分段|deepseek.com / API|

|模型|适合短篇小说的亮点|明显短板|一句话使用建议|
|:--|:--|:--|:--|
|**Qwen3**|中文语感最好，一次可出 8K–12K token 完整初稿，擅长大纲→正文→润色一条龙|英文或跨文化场景略弱|纯中文、爽文/都市题材首选|
|**Gemini**|100 万 token 超长窗口，可一次性把整部短篇+改写历史全塞进去做全局修订|单次输出仍被锁 65K，长文需手动分段；中文语感一般|需要“整本投喂再整体改写”时用它|
|**Grok**|实时联网+幽默口吻，热点科幻、赛博讽刺类短篇秒出梗|中文细节、人物心理深度不足；输出上限 8K 左右|写带时效梗的轻科幻/黑色幽默短篇|
|**GLM-4.5**|结构工整、逻辑闭环好，适合高概念或悬疑短篇|创意惊喜度一般，需要多轮提示放大脑洞|悬疑推理、高设定科幻可先让它打骨架|
好了，所以，我可以用GLM 4.5来打个框架，然后扔给QWEN生成全文，最后让Gemini作为编辑去检查润色？

Kimi肯定了我的想法，还给了我提示词。
1️⃣ GLM-4.5 搭骨架（高概念 + 悬念闭环）  
目标：让 GLM 用最少 token 生成「世界观 + 主线冲突 + 章节节点」，控制 800～1 200 token。  
示范 prompt：

```
【角色】你是一位悬疑小说结构师，擅长“高概念一句话+三幕九节点”法。  
【任务】请用中文给出：  
1. 高概念一句话（≤30字）  
2. 三幕九节点（每节点≤25字）  
3. 每章 100 字梗概（共 9 章）  
【题材】近未来科幻悬疑  
【关键词】记忆篡改、量子加密遗嘱  
【输出格式】Markdown 表格  
```
2️⃣ Qwen3 扩写成完整短篇（10 k token 以内一次完成）  
目标：把 9 章梗概一次性扩写成 8 000～10 000 字流畅正文。  
示范 prompt：

复制

```
【角色】你是一位中文都市科幻畅销作者，文风细腻、对白生活化。  
【输入】（直接贴 GLM 生成的九节点梗概）  
【要求】  
- 每章 900～1100 字，共 9 章  
- 第一章以“我醒来时，手机相册里多了一张我睡着时的照片”开局  
- 对白口语化，第三人称有限视角  
- 悬念递进，结尾留开放式钩子  
【输出】纯正文，不保留梗概标题  
```

3️⃣ Gemini 全局润色 + 一致性检查（100 万 token 窗口优势）  
目标：把原始正文 + 读者反馈（可选）一次性读进去，输出 65 k token 以内的精修稿。  
示范 prompt：

复制

```
【角色】你是一位资深科幻编辑，擅长删减冗余、强化悬念、统一时间线。  
【任务】  
1. 检查时间线、人名、科技设定是否自洽  
2. 删除重复信息，压缩 10 % 字数  
3. 在关键悬念处插入“倒计时”式短句  
4. 把口语化对白改为更凝练的短句  
【输入】（直接贴 Qwen3 全文）  
【输出】整篇润色后正文，不保留修改痕迹  
```

但是呢，在网页版上操作要注意：
1. 免费网页版的硬伤  
    • **GLM-4.5 网页端**：单次对话最多 4 k token（≈ 3 000 汉字），你让它出 9 节点骨架时别超 3 000 字即可；再长就得分两次。  
    • **Qwen3 网页端**：单次输出约 2 k token（≈ 1 500 汉字），想一次拿到 8–10 k 字必须手动“继续写”。  
    • **Gemini 网页端（AI Studio）**：虽然窗口号称 1 M，但免费层同时限制  
    ‑ 每分钟 60 次请求  
    ‑ 每分钟 2 万 token 总量  
    你把 Qwen 的 8 k 正文贴进去润色完全够，但**一次只能返回 8 k token**，超过就要“继续”。
    
2. 用网页也能跑通的“分段”法  
    ① GLM：让它先给 9 节点，每节点 25 字内，总长度 < 400 字，一次搞定。  
    ② Qwen：按节点逐章扩写，每轮 1 500 字左右，点 5～6 次“继续写”即可凑够 8 k+。  
    ③ Gemini：把 Qwen 全文一次性贴进去，如果返回被截断，直接说“继续润色”，它会接着上次的上下文输出。

好玩，咱们这就开始。当然，我的主题设置不是按照它的示例，我夹带私货。并且，我没让Gemini来润色。相反我把GLM生成的框架同时扔给了Deepseek，QWen3，Chatgpt，Grok，Gemini，Claude，甚至GLM自己和Kimi。我很有兴趣知道，它门之中谁有文豪的潜质。

生成的文章链接贴在文章底部，如果你们有兴趣，可以自己去阅读，评判，然后把你们的结论放在评论区告诉我。但就我而言，我的感受是：
Gemini：一百万的上下文不是盖的，轻松输出整篇小说。质量中上。
QWEN：同样上下文的尺寸大，一次性输出整篇，但质量一言难尽，我感觉垫底啊。
Deepseek：受限于整体上下文的大小和单次输出的限制，它一次性只输出一章。你必须不停的跟它说继续第二章，继续第三章，直到整篇都输出完毕。质量怎么说呢，开篇我感觉惊艳，但结尾感觉有些草率。也许是64K上下文的大小限制导致的？
Kimi 和GLM：输出的时候和Deepseek一样，必须不停的说继续。
Grok：也是要不停的说继续，但是它和其他大模型不同的地方是，它不是将文章输出在上下文，而是一个链接。点击可以打开侧栏。你可以拷贝也可以下载。小说质量一般，并且我感觉有两章由部分重复。
Claude 和 ChatGPT是输出最不顺的两个大模型了。
先说Claude：它似乎一次性尝试输出全文，但是上下文有限或者对我这个免费玩家的请求限制，输出到第三章就停了。我说几次说继续，它输出的长短不一，最后索性直接说不伺候了。我索性停了几个小时，晚上再继续，勉强输出完本。并且越到后面，就明显有逻辑问题。

Chatgpt在输出完第五章后，第六章开始输出我上一个话题的内容了。我当时就震惊了，一时不知道该怎么办。最后不得已，将它创作了一半的作品和提示词一起附上，才算完成了后面一半的创作。成文质量不错，与deepseek有得一拼。但也是最后有点虎头蛇尾。

好啦，如果你有兴趣，就请拜读一下几位大师的作品吧。

---

## 各大模型创作的小说作品

以下是各大模型根据统一框架创作的短篇小说，欢迎大家阅读并评论：

- [ChatGPT创作的小说](./Article_Chatgpt/)
- [Claude创作的小说](./Article_Claude/)
- [DeepSeek创作的小说](./Article_DS/)
- [Gemini创作的小说](./Article_Gemini/)
- [GLM创作的小说](./Article_GLM/)
- [Grok创作的小说](./Article_Grok/)
- [Kimi创作的小说](./Article_Kimi/)
- [Qwen创作的小说](./Article_QWEN/)
